{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-image\n",
    "# !pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps, ImageFilter, ImageEnhance\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate,interpolation as inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def convert_white_text_to_black(image):\n",
    "    # Convert to numpy array\n",
    "    image = np.array(image)\n",
    "    \n",
    "    # Define the threshold for \"white\" (you can adjust 240 depending on your needs)\n",
    "    threshold = 240\n",
    "    \n",
    "    # For grayscale images\n",
    "    if len(image.shape) == 2:  # Grayscale image\n",
    "        # Set pixels that are nearly white (greater than threshold) to black (0)\n",
    "        image[image >= threshold] = 0\n",
    "    else:  # Color image (RGB)\n",
    "        # Convert white pixels (where R, G, and B are all close to 255) to black\n",
    "        white_mask = np.all(image >= threshold, axis=-1)\n",
    "        image[white_mask] = [0, 0, 0]  # Set white pixels to black\n",
    "    \n",
    "    return Image.fromarray(image)\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `preprocessed_image` is the image you've already preprocessed (e.g., denoised):\n",
    "# preprocessed_image = convert_white_text_to_black(preprocessed_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skew correctness\n",
    "def skew_correctness(img):\n",
    "    \n",
    "    # convert to binary\n",
    "    wd, ht = img.size\n",
    "    pix = np.array(img.convert('1').getdata(), np.uint8)\n",
    "    bin_img = 1 - (pix.reshape((ht, wd)) / 255.0)\n",
    "    plt.imshow(bin_img, cmap='gray')\n",
    "    plt.savefig('binary.png')\n",
    "    def find_score(arr, angle):\n",
    "        data = inter.rotate(arr, angle, reshape=False, order=0)\n",
    "        hist = np.sum(data, axis=1)\n",
    "        score = np.sum((hist[1:] - hist[:-1]) ** 2)\n",
    "        return hist, score\n",
    "    delta = 1\n",
    "    limit = 5\n",
    "    angles = np.arange(-limit, limit+delta, delta)\n",
    "    scores = []\n",
    "    for angle in angles:\n",
    "        hist, score = find_score(bin_img, angle)\n",
    "        scores.append(score)\n",
    "    best_score = max(scores)\n",
    "    best_angle = angles[scores.index(best_score)]\n",
    "    \n",
    "    # correct skew\n",
    "    data = inter.rotate(bin_img, best_angle, reshape=False, order=0)\n",
    "    img = Image.fromarray((255 * data).astype(\"uint8\"))\n",
    "    img.save('skew_corrected.png')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarization(image):\n",
    "    open_cv_image = np.array(image)\n",
    "    ret, imgf = cv2.threshold(open_cv_image, 0, 255,cv2.THRESH_BINARY,cv2.THRESH_OTSU)\n",
    "    return Image.fromarray(imgf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_image(image, threshold=128):\n",
    "    \"\"\"\n",
    "    Convert an image to binary (black and white) using a threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "    - threshold: Pixel value threshold for binarization (default is 128)\n",
    "\n",
    "    Returns:\n",
    "    - Binarized PIL Image object\n",
    "    \"\"\"\n",
    "    return image.point(lambda x: 0 if x < threshold else 255, '1')  # Convert pixels below the threshold to black\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoised_image(image):\n",
    "    image=np.array(image)\n",
    "    \n",
    "    return Image.fromarray(cv2.fastNlMeansDenoising(image, None, 30, 7, 21))\n",
    "    \n",
    "    # return Image.fromarray(cv2.fastNlMeansDenoising(image, h=30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_thresholding(image):\n",
    "    \"\"\"\n",
    "    Apply adaptive thresholding to the image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "\n",
    "    Returns:\n",
    "    - PIL Image object after adaptive thresholding\n",
    "    \"\"\"\n",
    "    # Convert the PIL Image to OpenCV format\n",
    "    open_cv_image = np.array(image)\n",
    "    \n",
    "    # Apply adaptive thresholding using OpenCV\n",
    "    thresholded = cv2.adaptiveThreshold(\n",
    "        open_cv_image,\n",
    "        255,\n",
    "        cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "        cv2.THRESH_BINARY,\n",
    "        11,  # Block size\n",
    "        2    # Constant to subtract from mean\n",
    "    )\n",
    "    \n",
    "    # Convert back to PIL Image format\n",
    "    return Image.fromarray(thresholded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, scale_factor=3):\n",
    "    \"\"\"\n",
    "    Resize the image by a given scale factor.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "    - scale_factor: Factor by which to scale the image (default is 3x)\n",
    "\n",
    "    Returns:\n",
    "    - Resized PIL Image object\n",
    "    \"\"\"\n",
    "    new_size = (int(image.width * scale_factor), int(image.height * scale_factor))\n",
    "    resized_image = image.resize(new_size, Image.LANCZOS)  # Use LANCZOS for high-quality resizing\n",
    "    return resized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_image(image):\n",
    "    return ImageOps.equalize(image) \n",
    "\n",
    "def binarize_image(image, threshold=32):\n",
    "    \"\"\"\n",
    "    Convert an image to binary (black and white) using a threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "    - threshold: Pixel value threshold for binarization (default is 128)\n",
    "\n",
    "    Returns:\n",
    "    - Binarized PIL Image object\n",
    "    \"\"\"\n",
    "    return image.point(lambda x: 0 if x < threshold else 255, '1')  # Convert pixels below the threshold to black\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sharpen_image(image):\n",
    "    \"\"\"\n",
    "    Sharpen the image to enhance text edges.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "\n",
    "    Returns:\n",
    "    - Sharpened PIL Image object\n",
    "    \"\"\"\n",
    "    return image.filter(ImageFilter.SHARPEN)\n",
    "\n",
    "def adjust_brightness_contrast(image, brightness_factor=2, contrast_factor=3):\n",
    "    \"\"\"\n",
    "    Adjust brightness and contrast of the image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "    - brightness_factor: Factor to adjust brightness (default is 1.5)\n",
    "    - contrast_factor: Factor to adjust contrast (default is 2)\n",
    "\n",
    "    Returns:\n",
    "    - Adjusted PIL Image object\n",
    "    \"\"\"\n",
    "    # Adjust brightness\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(brightness_factor)\n",
    "    \n",
    "    # Adjust contrast\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(contrast_factor)\n",
    "    \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sharpen_image1(image):\n",
    "    # Create a sharpening kernel\n",
    "    image=np.array(image)\n",
    "    kernel = np.array([[0, -1, 0], \n",
    "                       [-1, 5,-1], \n",
    "                       [0, -1, 0]])\n",
    "    \n",
    "    # Apply the kernel to the image\n",
    "    sharpened = cv2.filter2D(src=image, ddepth=-1, kernel=kernel)\n",
    "    return Image.fromarray(sharpened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    \"\"\"\n",
    "    Normalize the pixel values of an image to the range 0-255.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL Image object\n",
    "\n",
    "    Returns:\n",
    "    - Normalized PIL Image object\n",
    "    \"\"\"\n",
    "    # Convert PIL Image to numpy array\n",
    "    image_np = np.array(image).astype(np.float32)\n",
    "\n",
    "    # Normalize to 0-1 range\n",
    "    normalized = (image_np - np.min(image_np)) / (np.max(image_np) - np.min(image_np))\n",
    "\n",
    "    # Scale back to 0-255 and convert to uint8\n",
    "    normalized = (normalized * 255).astype(np.uint8)\n",
    "\n",
    "    # Convert back to PIL Image\n",
    "    return Image.fromarray(normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.filters import unsharp_mask\n",
    "\n",
    "# def sharpen_image(image):\n",
    "#     \"\"\"\n",
    "#     Sharpen the image using unsharp mask from skimage.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image: PIL Image object\n",
    "\n",
    "#     Returns:\n",
    "#     - Sharpened PIL Image object\n",
    "#     \"\"\"\n",
    "#     # Convert image to numpy array and normalize to 0-1 range\n",
    "#     image_np = np.array(image).astype(np.float32) / 255.0\n",
    "    \n",
    "#     # Apply unsharp mask\n",
    "#     sharp_image_np = unsharp_mask(image_np, radius=1.0, amount=0.5)\n",
    "    \n",
    "#     # Convert back to 0-255 range and uint8 format\n",
    "#     sharp_image_np = np.clip(sharp_image_np * 255.0, 0, 255).astype(np.uint8)\n",
    "    \n",
    "#     # Convert back to PIL Image\n",
    "#     return Image.fromarray(sharp_image_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Blind Deconvolution\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam\n",
    "# import numpy as np\n",
    "\n",
    "# def blind_deconvolution(blurred_image, num_iterations=1000, kernel_size=5):\n",
    "#     # Convert image to torch tensor\n",
    "#     blurred_image=np.array(blurred_image)\n",
    "#     blurred_tensor = torch.tensor(blurred_image, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "#     # Initialize kernel and latent image\n",
    "#     kernel = torch.ones((1, 1, kernel_size, kernel_size), requires_grad=True)\n",
    "#     latent_image = torch.randn_like(blurred_tensor, requires_grad=True)\n",
    "\n",
    "#     # Define optimizer\n",
    "#     optimizer = Adam([latent_image, kernel], lr=0.01)\n",
    "\n",
    "#     for i in range(num_iterations):\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Convolve the latent image with the kernel\n",
    "#         blurred_estimate = F.conv2d(latent_image, kernel, padding=kernel_size//2)\n",
    "        \n",
    "#         # Compute the loss between observed blurred image and estimate\n",
    "#         loss = F.mse_loss(blurred_estimate, blurred_tensor)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Normalize kernel to ensure it's a valid PSF\n",
    "#         with torch.no_grad():\n",
    "#             kernel.clamp_(min=0)\n",
    "#             kernel /= kernel.sum()\n",
    "\n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
    "\n",
    "#     # Return the final estimated latent image and kernel\n",
    "#     return latent_image.detach().squeeze().numpy(), kernel.detach().squeeze().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoising(image):\n",
    "    image=np.array(image)\n",
    "    # denoising of image saving it into dst image \n",
    "    dst = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 15) \n",
    "    # Plotting of source and destination image \n",
    "    return Image.fromarray(dst)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinning(image):\n",
    "    image=np.array(image)\n",
    "    kernel = np.ones((2,2),np.uint8)\n",
    "    erosion = cv2.erode(image,kernel,iterations = 1)\n",
    "    return Image.fromarray(erosion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_image_for_ocr(image_path, scale_factor=5, threshold=10, brightness_factor=1.5, contrast_factor=2):\n",
    "    \"\"\"\n",
    "    Preprocess the image for optimal OCR performance.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: Path to the input image file\n",
    "    - scale_factor: Factor by which to scale the image (default is 3x)\n",
    "    - threshold: Pixel value threshold for binarization (default is 50)\n",
    "    - brightness_factor: Factor to adjust brightness (default is 1.5)\n",
    "    - contrast_factor: Factor to adjust contrast (default is 2)\n",
    "\n",
    "    Returns:\n",
    "    - Preprocessed PIL Image object\n",
    "    \"\"\"\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    # image=denoising(image)\n",
    "    image = image.convert('L')\n",
    "    \n",
    "    \n",
    "    image = sharpen_image(image)    \n",
    "\n",
    "    # Resize the image\n",
    "    image = resize_image(image, scale_factor)   \n",
    "    \n",
    "    # Convert to grayscale\n",
    "    image=denoised_image(image)\n",
    "\n",
    "    #blind dev\n",
    "    # image, estimated_kernel = blind_deconvolution(image)\n",
    "    # image = Image.fromarray(np.uint8(image * 255)) \n",
    "    #Equlizes the image\n",
    "    image=equalize_image(image)\n",
    "\n",
    "    # Normlize the image    \n",
    "    image=normalize(image)\n",
    "    \n",
    "    \n",
    "#    #Sharpen the image\n",
    "   \n",
    "   \n",
    "    \n",
    "#     # Adjust brightness and contrast\n",
    "    image = adjust_brightness_contrast(image, brightness_factor, contrast_factor)\n",
    "    \n",
    "    # Sharpen the image\n",
    "    \n",
    "    #Thinning\n",
    "    \n",
    "    \n",
    "    # Apply binary thresholding\n",
    "    \n",
    "    # image=adaptive_thresholding(image)\n",
    "    # image=binarization(image)\n",
    "    image=thinning(image)\n",
    "    image=convert_white_text_to_black(image)\n",
    "    image = binarize_image(image, threshold)    \n",
    "    # image=skew_correctness(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/snap/core20/current/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /lib/x86_64-linux-gnu/libproxy.so.1)\n",
      "Failed to load module: /home/nova/snap/code/common/.cache/gio-modules/libgiolibproxy.so\n",
      "Gtk-Message: 10:46:46.028: Failed to load module \"appmenu-gtk-module\"\n",
      "eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    }
   ],
   "source": [
    "image_path = '../images/41ADVPQgZOL.jpg'\n",
    "image_path1 = '../images/51KykmLgc0L.jpg'\n",
    "image_path2 = '../images/417SThj+SrL.jpg'\n",
    "image_path3 = '../images/41-NCxNuBxL.jpg'\n",
    "image_path4 = '../images/81aZ2ozp1GL.jpg'\n",
    "preprocessed_image = preprocess_image_for_ocr(image_path3)\n",
    "preprocessed_image.show()  # Show the preprocessed image\n",
    "preprocessed_image.save(\"pre.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from skimage.restoration import denoise_tv_chambolle\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# # Function to apply unsharp mask (enhances the edges and details)\n",
    "# def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=1.5, threshold=0):\n",
    "#     blurred = cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "#     sharpened = float(amount + 1) * image - float(amount) * blurred\n",
    "#     sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n",
    "#     sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n",
    "#     sharpened = sharpened.round().astype(np.uint8)\n",
    "#     return sharpened\n",
    "\n",
    "# # Function to preprocess a blurry image\n",
    "# def preprocess_blurry_image(image_path):\n",
    "#     # Load the image\n",
    "#     img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    \n",
    "#     # Step 1: Apply unsharp mask for edge enhancement\n",
    "#     sharpened_img = unsharp_mask(img)\n",
    "    \n",
    "#     # Step 2: Apply Total Variation (TV) denoising for deblurring\n",
    "#     tv_deblurred_img = denoise_tv_chambolle(sharpened_img, weight=0.1)\n",
    "    \n",
    "#     # Rescale to 0-255 after TV deblurring\n",
    "#     tv_deblurred_img = (tv_deblurred_img * 255).astype(np.uint8)\n",
    "    \n",
    "#     # Step 3: Apply CLAHE for contrast enhancement\n",
    "#     clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "#     enhanced_img = clahe.apply(tv_deblurred_img)\n",
    "\n",
    "#     # Step 4: Apply adaptive thresholding for binarization\n",
    "#     # threshold_img = cv2.adaptiveThreshold(\n",
    "#     #     enhanced_img, 255, \n",
    "#     #     cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "#     #     cv2.THRESH_BINARY, \n",
    "#     #     11, 2\n",
    "#     # )\n",
    "#     _,threshold_img = cv2.threshold(enhanced_img, 0, 255,cv2.THRESH_BINARY,cv2.THRESH_OTSU)\n",
    "\n",
    "#     # Step 5: Optional dilation and erosion to improve character shapes\n",
    "#     kernel = np.ones((2, 2), np.uint8)\n",
    "#     processed_img = cv2.dilate(threshold_img, kernel, iterations=1)\n",
    "#     processed_img = cv2.erode(processed_img, kernel, iterations=1)\n",
    "\n",
    "#     # Step 6: Save and display the processed image\n",
    "#     processed_image_path = 'processed_image.png'\n",
    "#     cv2.imwrite(processed_image_path, processed_img)\n",
    "    \n",
    "#     # Display for verification\n",
    "#     plt.imshow(processed_img, cmap='gray')\n",
    "#     plt.title('Preprocessed Image for OCR')\n",
    "#     plt.show()\n",
    "\n",
    "#     return processed_image_path\n",
    "\n",
    "# # Example usage\n",
    "# preprocessed_image = preprocess_blurry_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during super resolution: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"ESPCN_x4.pb\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# # Function to upscale the image using super resolution\n",
    "# def super_resolution(image_path):\n",
    "#     try:\n",
    "#         # Load the pre-trained ESPCN model for super resolution\n",
    "#         sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "#         model_path = 'ESPCN_x4.pb'  # Path to ESPCN model\n",
    "#         sr.readModel(model_path)\n",
    "#         sr.setModel('espcn', 4)  # Set the model and scale factor\n",
    "\n",
    "#         # Read the image\n",
    "#         img = cv2.imread(image_path)\n",
    "#         if img is None:\n",
    "#             raise FileNotFoundError(f\"Image not found at {image_path}\")\n",
    "\n",
    "#         # Upscale the image\n",
    "#         upscaled_img = sr.upsample(img)\n",
    "\n",
    "#         # Convert to grayscale after upscaling\n",
    "#         gray_upscaled_img = cv2.cvtColor(upscaled_img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#         # Save and display the upscaled image\n",
    "#         upscaled_image_path = 'upscaled_image.png'\n",
    "#         cv2.imwrite(upscaled_image_path, gray_upscaled_img)\n",
    "\n",
    "#         plt.imshow(gray_upscaled_img, cmap='gray')\n",
    "#         plt.title('Super Resolution Image')\n",
    "#         plt.show()\n",
    "\n",
    "#         return upscaled_image_path\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during super resolution: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Function to preprocess the super-resolution image\n",
    "# def preprocess_super_res_image(image_path):\n",
    "#     try:\n",
    "#         # Step 1: Apply Super Resolution\n",
    "#         upscaled_image_path = super_resolution(image_path)\n",
    "#         if upscaled_image_path is None:\n",
    "#             return None\n",
    "\n",
    "#         # Step 2: Load the upscaled image\n",
    "#         img = cv2.imread(upscaled_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "#         if img is None:\n",
    "#             raise FileNotFoundError(f\"Upscaled image not found at {upscaled_image_path}\")\n",
    "\n",
    "#         # Step 3: Apply CLAHE for contrast enhancement\n",
    "#         clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "#         enhanced_img = clahe.apply(img)\n",
    "\n",
    "#         # Step 4: Apply Adaptive Thresholding\n",
    "#         threshold_img = cv2.adaptiveThreshold(\n",
    "#             enhanced_img, 255,\n",
    "#             cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "#             cv2.THRESH_BINARY, 11, 2\n",
    "#         )\n",
    "\n",
    "#         # Step 5: Dilation and Erosion for text emphasis\n",
    "#         kernel = np.ones((2, 2), np.uint8)\n",
    "#         processed_img = cv2.dilate(threshold_img, kernel, iterations=1)\n",
    "#         processed_img = cv2.erode(processed_img, kernel, iterations=1)\n",
    "\n",
    "#         # Save and display the processed image\n",
    "#         processed_image_path = 'processed_super_res_image.png'\n",
    "#         cv2.imwrite(processed_image_path, processed_img)\n",
    "\n",
    "#         plt.imshow(processed_img, cmap='gray')\n",
    "#         plt.title('Preprocessed Super Resolution Image for OCR')\n",
    "#         plt.show()\n",
    "\n",
    "#         return processed_image_path\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during preprocessing: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Example usage\n",
    "# # image_path = 'your_image_path_here.png'  # Define the actual image path\n",
    "# preprocessed_image = preprocess_super_res_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-contrib-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "# # !pip install torch torchvision opencv-python pillow\n",
    "\n",
    "# import cv2\n",
    "# print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from skimage import data, restoration\n",
    "# from scipy.signal import convolve2d\n",
    "\n",
    "# # Load the example camera image (already in grayscale)\n",
    "# image = Image.open(image_path)\n",
    "# image = Image.open(image_path).convert('L')\n",
    "# image=  np.array(image)\n",
    "\n",
    "# # Create a synthetic blur kernel (Point Spread Function - PSF)\n",
    "# psf = np.ones((5, 5)) / 25  # Simple average (motion blur example)\n",
    "# blurred_image = convolve2d(image, psf, 'same')\n",
    "\n",
    "# # Add some noise to simulate a real-world scenario\n",
    "# noisy_blurred_image = blurred_image + 0.1 * blurred_image.std() * np.random.standard_normal(blurred_image.shape)\n",
    "\n",
    "# # Apply Richardson-Lucy deconvolution\n",
    "# deconvolved_image = restoration.richardson_lucy(noisy_blurred_image, psf, num_iter=30)\n",
    "\n",
    "# # Display results\n",
    "# plt.figure(figsize=(10, 5))\n",
    "\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.title('Original Image')\n",
    "# plt.imshow(image, cmap='gray')\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.title('Blurred + Noisy Image')\n",
    "# plt.imshow(noisy_blurred_image, cmap='gray')\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.title('Deconvolved Image (Richardson-Lucy)')\n",
    "# plt.imshow(deconvolved_image, cmap='gray')\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 51jTe522S2L.jpg and saved to processed_images/51jTe522S2L.jpg\n",
      "Processed 51BEuVR4ZzL.jpg and saved to processed_images/51BEuVR4ZzL.jpg\n",
      "Processed 71UN1IxKp4L.jpg and saved to processed_images/71UN1IxKp4L.jpg\n",
      "Processed 41nblnEkJ3L.jpg and saved to processed_images/41nblnEkJ3L.jpg\n",
      "Processed 417NJrPEk+L.jpg and saved to processed_images/417NJrPEk+L.jpg\n",
      "Processed pre.png and saved to processed_images/pre.png\n",
      "Processed 51cPZYLk2YL.jpg and saved to processed_images/51cPZYLk2YL.jpg\n",
      "Processed 51KykmLgc0L.jpg and saved to processed_images/51KykmLgc0L.jpg\n",
      "Processed 81qUmRUUTTL.jpg and saved to processed_images/81qUmRUUTTL.jpg\n",
      "Processed 51+oHGvSvuL.jpg and saved to processed_images/51+oHGvSvuL.jpg\n",
      "Processed 51vwYpDz2tL.jpg and saved to processed_images/51vwYpDz2tL.jpg\n",
      "Processed 51tEop-EBJL.jpg and saved to processed_images/51tEop-EBJL.jpg\n",
      "Processed 41ADVPQgZOL.jpg and saved to processed_images/41ADVPQgZOL.jpg\n",
      "Processed 61C+fwVD6dL.jpg and saved to processed_images/61C+fwVD6dL.jpg\n",
      "Processed pre2.png and saved to processed_images/pre2.png\n",
      "Processed 81IYdOV0mVL.jpg and saved to processed_images/81IYdOV0mVL.jpg\n",
      "Processed 71ta6wY3HtL.jpg and saved to processed_images/71ta6wY3HtL.jpg\n",
      "Processed 613P5cxQH4L.jpg and saved to processed_images/613P5cxQH4L.jpg\n",
      "Processed 71v+pim0lfL.jpg and saved to processed_images/71v+pim0lfL.jpg\n",
      "Processed 51kdBAv6ImL.jpg and saved to processed_images/51kdBAv6ImL.jpg\n",
      "Processed 51EBBqNOJ1L.jpg and saved to processed_images/51EBBqNOJ1L.jpg\n",
      "Processed 514pScQdlCL.jpg and saved to processed_images/514pScQdlCL.jpg\n",
      "Processed 615Cjzm6pyL.jpg and saved to processed_images/615Cjzm6pyL.jpg\n",
      "Processed 51l6c6UcRZL.jpg and saved to processed_images/51l6c6UcRZL.jpg\n",
      "Processed 51oaOP8qJlL.jpg and saved to processed_images/51oaOP8qJlL.jpg\n",
      "Processed 51FSlaVlejL.jpg and saved to processed_images/51FSlaVlejL.jpg\n",
      "Processed 61O+Yi09tyL.jpg and saved to processed_images/61O+Yi09tyL.jpg\n",
      "Processed 71fWddA0+yL.jpg and saved to processed_images/71fWddA0+yL.jpg\n",
      "Processed 41o3iis9E7L.jpg and saved to processed_images/41o3iis9E7L.jpg\n",
      "Processed 71WAjPMQDWL.jpg and saved to processed_images/71WAjPMQDWL.jpg\n",
      "Processed 51Su6zXkAsL.jpg and saved to processed_images/51Su6zXkAsL.jpg\n",
      "Processed 417SThj+SrL.jpg and saved to processed_images/417SThj+SrL.jpg\n",
      "Processed 51fAzxNm+cL.jpg and saved to processed_images/51fAzxNm+cL.jpg\n",
      "Processed 61G8bvWOb-L.jpg and saved to processed_images/61G8bvWOb-L.jpg\n",
      "Processed 61E2XRNSdYL.jpg and saved to processed_images/61E2XRNSdYL.jpg\n",
      "Processed 614hn5uX9MS.jpg and saved to processed_images/614hn5uX9MS.jpg\n",
      "Processed 51P0IuT6RsL.jpg and saved to processed_images/51P0IuT6RsL.jpg\n",
      "Processed 71UYDq4nfnL.jpg and saved to processed_images/71UYDq4nfnL.jpg\n",
      "Processed 41-NCxNuBxL.jpg and saved to processed_images/41-NCxNuBxL.jpg\n",
      "Processed 71eCfiIG-AL.jpg and saved to processed_images/71eCfiIG-AL.jpg\n",
      "Processed 51y79cwGJFL.jpg and saved to processed_images/51y79cwGJFL.jpg\n",
      "Processed 41zgjN+zW3L.jpg and saved to processed_images/41zgjN+zW3L.jpg\n",
      "Processed 41ygXRvf8lL.jpg and saved to processed_images/41ygXRvf8lL.jpg\n",
      "Processed 510xYFNYQ8L.jpg and saved to processed_images/510xYFNYQ8L.jpg\n",
      "Processed 71Qk6hR9-WL.jpg and saved to processed_images/71Qk6hR9-WL.jpg\n",
      "Processed 51-WIOx5pxL.jpg and saved to processed_images/51-WIOx5pxL.jpg\n",
      "Processed 41uwo4PVnuL.jpg and saved to processed_images/41uwo4PVnuL.jpg\n",
      "Processed 71afEPoRGsL.jpg and saved to processed_images/71afEPoRGsL.jpg\n",
      "Processed 61lX6IP1SVL.jpg and saved to processed_images/61lX6IP1SVL.jpg\n",
      "Processed 81PG3ea0MOL.jpg and saved to processed_images/81PG3ea0MOL.jpg\n",
      "Processed 41pvwR9GbaL.jpg and saved to processed_images/41pvwR9GbaL.jpg\n",
      "Processed 51bEy0J5wLL.jpg and saved to processed_images/51bEy0J5wLL.jpg\n",
      "Processed 514bY8c4ZIL.jpg and saved to processed_images/514bY8c4ZIL.jpg\n",
      "Processed 81aZ2ozp1GL.jpg and saved to processed_images/81aZ2ozp1GL.jpg\n",
      "Processed 51r7U52rh7L.jpg and saved to processed_images/51r7U52rh7L.jpg\n",
      "Processed 51H+mX2Wk7L.jpg and saved to processed_images/51H+mX2Wk7L.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def process_images_in_folder(input_folder, output_folder):\n",
    "    \"\"\"\n",
    "    Process all images in the input folder and save them to the output folder.\n",
    "\n",
    "    Parameters:\n",
    "    - input_folder: Path to the folder containing input images\n",
    "    - output_folder: Path to the folder where processed images will be saved\n",
    "    \"\"\"\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Iterate over all files in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            processed_image = preprocess_image_for_ocr(image_path)\n",
    "            processed_image_path = os.path.join(output_folder, filename)\n",
    "            processed_image.save(processed_image_path)\n",
    "            print(f\"Processed {filename} and saved to {processed_image_path}\")\n",
    "\n",
    "# Define the paths for input and output folders\n",
    "input_folder = '../images'\n",
    "output_folder = 'processed_images'\n",
    "\n",
    "# Process all images in the input folder\n",
    "process_images_in_folder(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm_img = np.zeros((img.shape[0], img.shape[1]))\n",
    "# img = cv2.normalize(img, norm_img, 0, 255, cv2.NORM_MINMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
